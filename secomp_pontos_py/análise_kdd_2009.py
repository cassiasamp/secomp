# -*- coding: utf-8 -*-
"""Análise KDD 2009.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Y3M0zwbRnEwbSwkcWacRHy5fsuOC6s8

# O que é o desafio da Orange telecom? 
---

Explicando churn, appetency, up-selling e machine learning.

>Você já ouviu falar daquele aplicativo chamado Pokemon Go? 

>O Pokemon Go teve um hype no mercado, só que para pegar os pokemons era necessário sair andando pelas ruas, e muitas pessoas precisavam, por exemplo, passar 8 horas dentro de um escritório. Quando elas percebiam isso, rapidamente desinstalavam o app e cancelavam a sua conta.

>Quando você baixa um app, acaba usando o serviço por 3 dias e, por algum motivo, cancela a conta e o desinstala, isso tem um nome dentro da área de marketing das empresas, chama-se **taxa de rotatividade** ou **churn rate**.

>Agora vamos pensar em outra situação. 

>Se você já usou Uber, você sabe que também tem um serviço do Uber chamado Uber eats, certo? O que aconteceu com o Uber eats, foi que várias pessoas que gostavam do Uber começaram a usá-lo. 

>Quando você já gosta de um serviço e começa a utilizar outros serviços da mesma empresa, para o marketing isso também tem um nome, que significa o quanto aquele produto ou serviço agrada o seu paladar, é a sua **apetência** ou **appetency**.

>Além de churn rate e appetency, podemos ter outra situação.

>Você deve ter ou conhecer alguém que tem conta na Nubank. Quando você se cadastra nesse banco digital, eles te dão a opção de participar de um programa de recompensas para que você ganhe pontos. Para fazer esse cadastro, você precisa pagar uma taxa adicional pelo serviço. Mas, é claro, você só vai estar disposto a comprar mais alguma coisa caso esteja satisfeito com as funções básicas que o banco oferece.

>Esse tipo de oferta também tem um nome dentro do marketing, são as vendas adicionais ou **up-selling**.

>Agora que você já conhece churn rate, appetency e up-selling e sabe que são algumas métricas sobre clientes analisadas pelo departamento de marketing, podemos falar de uma empresa de telecom chamada **Orange**.

>Essa empresa lançou um desafio em 2009 que envolvia essas taxas. Eles queriam descobrir, para algumas contas, quais as probabilidades de seus clientes terem churn, de seus produtos terem appetency e de fazer up-selling. 

>Só que esses números que eles querem descobrir não estão em uma planilha, então iremos usar dados antigos de outras contas que já temos para tentar advinhar quais são as taxas futuras. E faremos isso usando um computador para encontrá-las.

>Logo, a máquina vai entender quais são esses padrões de clientes, e, para isso, vamos usar **machine learning** ou aprendizagem de máquina para ensiná-la.

# Obtendo des-zipando os dados

---
Explicando wget e unzip.

>O primeiro passo para ensinar a máquina é usar algo que ela entenda: números. Precisamos de dados sobre esses clientes e de dados sobre essas três taxas.

>A Orange disponibilizou esses dados no desafio, mas como os dados estão em um site, precisamos baixá-los. Para isso vamos usar o comando **!wget**.

>Repare que alguns dos dados estão zipados, então também vamos extrair os arquivos com o comando **!unzip**.
"""

!wget https://www.kdd.org/cupfiles/KDDCupData/2009/orange_small_train.data.zip https://www.kdd.org/cupfiles/KDDCupData/2009/orange_small_train_appetency.labels https://www.kdd.org/cupfiles/KDDCupData/2009/orange_small_train_churn.labels https://www.kdd.org/cupfiles/KDDCupData/2009/orange_small_train_upselling.labels https://www.kdd.org/cupfiles/KDDCupData/2009/orange_small_test.data.zip

!unzip -o orange_small_train.data.zip 
!unzip -o orange_small_test.data.zip

"""# Fazendo o import das bibliotecas


---
Explicando sobre as bibliotecas e pacotes do python.

>Agora que já temos os dados, precisamos achar um jeito de ensinar a máquina a lê-los, mas não adianta falarmos com ela em português, temos que usar uma linguagem que ela entenda, então faremos isso usando uma linguagem de programação chamada **Python**.

>O Python tem diversas coleções de códigos prontos ou bibliotecas que já podemos usar para fazer isso, uma delas é chamada de **Pandas**. 

>Pandas é uma biblioteca do Python cujo foco é nos ajudar na análise de dados. 

>Para usá-la precisamos importá-la com **import** e vamos dar um apelido para ela também que é **pd**.

>Ainda, como o pandas usa outra biblioteca chamada **numpy**, vamos importá-la também como **np**.
"""

import pandas as pd
import numpy as np

"""# Lendo e guardando os dados

---


Usando pandas para ler os dados de treino, de destino e os guardando nas variáveis.

>Legal, agora podemos usar pd quando precisamos de algo do pandas. 

>A primeira coisa que vamos fazer é pedir para ele ler os arquivos que baixamos com **pd.read_csv**. 

>Você pode ter estranhado isso, pois o nosso arquivo tem extensão **.data** e não **.csv**. Mas mesmo que o nosso arquivo não seja do tipo csv, podemos abri-lo e ver que ele é delimitado por **tab**, usando **sep='\t'**. 

>Desse modo, passamos um separador, fazendo com que o pandas o leia direitinho.

>Os primeiros dados que temos são os das contas dos clientes da Orange devidamente anonimizados, e, para dizer quais clientes dessas contas deram churn, tiveram upselling ou appetency, temos também as marcações ou **labels**.

>Então, temos os dados de contas e as marcações. Vamos ler e salvar todos eles.
"""

dados_contas = pd.read_csv('orange_small_train.data', sep='\t')

marcacoes_upselling = pd.read_csv('orange_small_train_upselling.labels', header=None)
marcacoes_appetency = pd.read_csv('orange_small_train_appetency.labels', header=None)
marcacoes_churn = pd.read_csv('orange_small_train_churn.labels', header=None)

"""# Explorando os dados


---

Obtendo informações sobre os dados de contas e os dados de destino.

>Bacana, agora que temos todos os nossos dados, vamos enteder melhor quais são eles, ou explorá-los. 

>Vamos começar pelos dados das contas.

>Nesse ponto, o pandas também vai nos ajudar com algumas funções. A primeira que vamos usar será para obter informações sobre esses dados. 

> Veja que usando **.info()** sabemos que temos 50.000 entradas, 230 colunas e tipos de dados variados entre ponto flutuante, inteiros e objetos.

## Dados de contas
"""

dados_contas.info()

""">Além disso, podemos olhar para esses dados, mas não precisamos carregar todas as 50000 entradas. 

>Fazemos isso usando **.head()** e passando o número de linhas que queremos ver, vou passar 5.
"""

dados_contas.head(5)

""">Repare que ao fazermos isso, podemos perceber que há dados que possuem um valor que não conseguimos definir ou representar, dizemos que esses valores **não são um número**, em inglês, **Not a Number**, o que é abreviado para **NaN**.

## Plotando a quantidade de NaN

>Como uma máquina entende números, pode ser que esses valores nos causem problemas. Mas será que eles são muitos ou poucos?

>Podemos ter uma idéia da sua quantidade usando um gráfico ou visualização. Eu escolhi usar um mapa que vai nos mostrar quais as áreas com mais ou menos NaN, chamado **heatmap**.

>Para fazermos o heatmap vamos usar outras bibliotecas e pacotes do python, como **seaborn** e **matplotlib**, então vamos importá-las.
"""

import seaborn as sns; sns.set()
import matplotlib.pyplot as plt

""">Além de importá-las, como estamos usando o notebook do Google collab, precisamos atualizar a versão do seaborn. 

>Para fazermos isso usamos o instalador de pacotes do python chamado **pip**, pedindo para que ele atualize o seaborn com **--upgrade** para a versão 0.9.0.
"""

print(sns.__version__)
!pip3 install --upgrade seaborn==0.9.0

""">Agora podemos começar a montar o nosso heatmap.

>Queremos saber qual a quantidade de NaNs, então vamos peguntar para o pandas se temos esses valores com **isnull()**, quando isso acontecer, o seaborn vai fazer com que a área do heatmap fique mais clara.
"""

sns.heatmap(dados_contas.isnull(), cbar=False)
plt.show()

""">Repare que grande parte das colunas como 0, 7, 14 até 185 são praticamente preenchidas com NaN, e como vamos fazer uma computação com um valor que não temos? 

>Perceba que ter vários NaN dificulta a nossa análise.

>Já demos uma olhada nos nossos dados de contas, agora vamos entender nossas marcações.

>Para isso, vamos também obter informações com .info, tanto para o up-selling quanto para appetency e churn.

>Veja que ao usarmos .info fica claro que temos também 50.000 dados, então que eles correspondem aos nossos dados de contas. 

>Repare também que, diferentemente dos dados das contas, todos os valores das marcações são inteiros, o que as faz uniformes no tipo.

## Upselling
"""

marcacoes_upselling.info()

""">Além de .info, podemos também descrever os nossos dados usando **.describe()**.

>Ao fazermos isso, precebemos que os valores mínimos das nossas labels são -1 e que os máximos são +1, o que mostra que eles variam entre -1 e 1.
"""

marcacoes_upselling.describe()

""">E é sempre uma boa idéia visualizar para sabermos como estão esses valores de -1 e 1, se estão mais ou menos balanceados. 

>Para isso, podemos fazer um gráfico com o seaborn chamado **distplot**.

>Ao fazermos o distplot percebemos que temos muito mais valores negativos do que positivos e que nossos dados não estão distribuídos de uma maneira uniforme.
"""

sns.distplot(marcacoes_upselling, kde = False)
plt.show()

""">Agora que já sabemos interpretar a visualização das marcações e a entendê-las com info e describe, vamos repetir o que fizemos para upselling para appetency e churn.

## Apettency
"""

sns.distplot(marcacoes_appetency, kde=False)
plt.show()

marcacoes_appetency.describe()

marcacoes_appetency.info()

"""## Churn"""

sns.distplot(marcacoes_churn, kde=False)
plt.show()

marcacoes_churn.describe()

marcacoes_churn.info()

""">Tendo explorado os nossos dados de contas e nossas marcações, nos dados de contas podemos ver que precisaremos dar um jeito nos valores indefinidos, naqueles que são object ou float e não inteiros  e também no imbalanceamento das marcações.

# Manipulando os dados


---

>Vamos começar com os dados de contas.

>Uma coisa que a Orange conta para a gente sobre os dados de contas do desafio é que as 190 primeiras entradas são de números e que as 40 últimas são de palavras ou nomes.

>Conseguimos usar os números na nossa análise caso eles não sejam NaN, mas não conseguimos usar as palavras. 

>Então vamos começar dividindo os dados de contas entre aqueles que tem números ou **numéricos** e os que tem nomes ou categorias, portanto, são **categóricos**.

>Para isso, vamos pegar as 190 primeiras colunas dos dados de contas e salvar como dados numéricos e as últimas 40 e salvar como dados categóricos.

>Para saber mais informações sobre as classificações dos dados, veja o meu post no blog da Alura: https://blog.alura.com.br/classificando-os-tipos-mais-comuns-de-dados/
"""

dados_numericos = dados_contas[dados_contas.columns[:190]]
dados_categoricos = dados_contas[dados_contas.columns[-40:]]

""">Podemos ter dividido os dados, mas ainda temos que lidar com os NaN. 

>Então vamos dar uma olhada no heatmap de cada um deles para entender como isso está. Repare que temos muitos NaN nos dados numéricos e também nos categóricos. O que será que vamos fazer com isso?
"""

sns.heatmap(dados_categoricos.isnull(), cbar=False)
plt.show()

sns.heatmap(dados_numericos.isnull(), cbar=False)
plt.show()

"""## Limpando os dados


---

>Uma saída é identificar onde estão esses NaN e trocá-los por alguma coisa que conseguimos entender.

>Ao fazermos isso estaríamos entrando com dados ou **imputando** dados no lugar de valores indefinidos. Para isso, temos uma estrutura chamada de **imputador simples** ou simple imputer numa biblioteca chamada **sklearn**.

>Vamos usar esse simple imputer para identificar os valores NaN e trocá-los por um valor constante. Estamos escolhendo essa estratégia pois temos colunas inteiras com NaN, e as outras estratégias descartam essas colunas quando elas são totalmente preenchidas por NaN. 

>Para configurar o simple imputer, passamos **np.nan** para que ele identifique os NaN como valores que faltam (ou **missing_values**) e constante (ou **constant**) como estratégia.

>Para aplicarmos o simple imputer, primeiro adequamos os NaN a nossa estratégia com **fit** e depois substituímos seus valores, transformando os dados com **transform**. Para agilizar esse processo, usaremos os dois juntos em **fit_transform**.

###Aplicando imputer para dados numéricos
"""

from sklearn.impute import SimpleImputer

imputer_para_dados_numericos = SimpleImputer(missing_values=np.nan, strategy='constant')
dados_numericos_com_imputer = imputer_para_dados_numericos.fit_transform(dados_numericos)

""">Como os nossos dados categóricos também possuem valores NaN, também iremos aplicar o simple imputer neles.

###Aplicando Simple Imputer para dados categóricos
"""

imputer_para_dados_categoricos = SimpleImputer(missing_values=np.nan, strategy='constant')
dados_categoricos_com_imputer = imputer_para_dados_categoricos.fit_transform(dados_categoricos)

""">Conseguimos resolver os NaN, mas o que podemos fazer com a diferença de tipos de dados que temos nos dados de contas?

>Vamos olhar para um exemplo. Repare que na coluna chamada **Var193** (caso não se lembre, basta digitar *dados_categoricos.head(5)*), nessa coluna temos quatro linhas em que está escrito "RO12" e uma na qual está escrito "AERks4I" e se pudéssemos também substrituir esses valores sem perdermos essas características?

>Para igualarmos os tipos de dados, poderíamos substituir "RO12" pelo número 5 e "AERks4I" por 3, por exemplo.

>Calma, não vamos calcular ou codificar as 40 colunas na mão, podemos usar uma outra estrutura do **sklearn** chamada codificador ou **encoder**.

### Aplicando Label Encoder para dados categóricos

>Para fazermos isso, vamos também usar uma função ou método chamado **fit_transform** só que vamos primeiro transformar os dados categóricos depois do imputer em uma estrutura de dados ou **DataFrame** do pandas. 

>Eu não contei isso para você antes, mas sempre que usamos o .head, .info e outras coisas do pandas, estamos mexendo em sua estrutura de data frame.
"""

df_dados_categoricos_com_imputer = pd.DataFrame(list(dados_categoricos_com_imputer))

""">Feito isso, como temos muitas colunas, precisamos passar cada uma delas pelo **encoder**. 

>Então escrevemos que para (ou **for**) cada índice de cada **coluna** (que é um item dos dados categóricos com encoder), cada uma dessas colunas é codificada.
"""

from sklearn.preprocessing import LabelEncoder

dados_categoricos_com_encoder = df_dados_categoricos_com_imputer.copy()

for indice,coluna in dados_categoricos_com_encoder.iteritems():
  dados_categoricos_com_encoder[indice] = LabelEncoder().fit_transform(coluna)

""">Desse modo, agora temos os nossos dados devidamente com valores numéricos. Você pode dar um .head() e conferir."""

dados_categoricos_com_encoder.head(5)

""">Previamente separamos os dados das contas em categóricos e numéricos, agora que tiramos os NaNs e codificamos os dados categóricos de maneira específica, vamos juntá-los novamente.

## Juntando os dados da conta

>Para isso, vamos pegar os nossos dados numéricos com imputer e os dados categóricos com encoder e juntá-los empilhando horizontalmente usando o **hstack** do numpy.

>Como manipulamos esses dados, vou chamá-los de dados de treino pré processados.

>Vou transformá-los em um dataframe do pandas para darmos uma olhada neles.
"""

dados_contas_pre_processados = np.hstack((dados_numericos_com_imputer, dados_categoricos_com_encoder))

df_dados_contas_pre_processados = pd.DataFrame(list(dados_contas_pre_processados))

df_dados_contas_pre_processados.head(5)

""">Olhando para os nossos dados, podemos perceber que temos valores dentro de uma faixa que vai de 0 até, por exemplo, 1610, ou até mais do que isso, como 5236. Sendo que 1610 e 5236 são números bem altos. 

>Repare que, se tivéssemos valores que fossem de 0 até 1, teríamos uma faixa com números menores, o que seria mais fácil de computar. Então, podemos processá-los mais um pouco para deixá-los entre 0 e 1 e facilitar para que a nossa máquina aprenda.

>Faremos isso usando um outro recurso do sklearn que é o **escalar** (de escala mesmo) de mínimos e máximos chamado **MinMax**. O que esse escalar vai fazer é deixar os nossos valores entre 0 e 1.

## Usando MinMax Scaler
"""

from sklearn.preprocessing import MinMaxScaler

escalar = scaler = MinMaxScaler(feature_range=(0, 1))
dados_contas_pre_processados_escalares = (escalar.fit_transform(dados_contas_pre_processados))

df_dados_contas_pre_processados = pd.DataFrame(list(dados_contas_pre_processados_escalares))

df_dados_contas_pre_processados.head(5)

""">Agora que nossos dados estão pré-processados, podemos começar a usá-los. Mas como? 

>Se vamos usar esse dados para ensinar a nossa máquina, podemos pensar em como a gente aprende. Se você for lá para a escola, temos aulas, praticamos o conteúdo e depois desse treino, fazemos um teste.

>Quando falamos de aprendizagem de máquina, é a mesma coisa. 

>Então de todos os nossos dados, vamos pegar uma parte deles que usaremos para treinar, ou **dados de treino**, e uma outra parte dos dados que serão usados para testá-lo, ou os  **dados de teste**. 

>Tanto os dados de treino quanto o de teste são subconjuntos dos dados de contas e das marcações.

>Mas repare em uma coisa, temos três marcações (ou targets): appetency, churn e upselling. O que significa que vamos prever três targets ao invés de uma, por isso, vamos fazer essa divisão para cada uma delas.

>Como estou usando um notebook e temos a parte visual, vou fazer as três aqui mesmo, mas você pode otimizar esse código fazendo uma função.

##Appetency


---

Dividindo os dados em treino e teste para appetency.

>Para fazer essa divisão vamos continuar usando o scikit-learn. Vou fazer o import do método **train_test_split** que já divide os dados para a gente em treino e teste.

>Comumente em machine learning ou data science, chamamos os dados de **X** e os targets de **y**, então a primeira coisa que vou fazer é passar os dados de contas para o nosso X e cada uma das marcações para o y.

>Agora vou usar o train_test_split para dividir o X e o y entre treino e teste. Mas como vou fazer essa divisão, não é mesmo? Consigo dizer para o método qual o tamanho do teste que eu quero, então vou dizer que vai ser um **test_size** de 33%, restando 77% para treino.

>Um detalhe importante para o método é fazer com que os nossos targets tenham uma dimensão, para isso vou usar uma função do numpy chamada **.ravel()**.
"""

from sklearn.model_selection import train_test_split

X = df_dados_contas_pre_processados.values
y_appetency = marcacoes_appetency.values

X_treino_appetency, X_teste_appetency, y_treino_appetency, y_teste_appetency = train_test_split(X, y_appetency.ravel(), test_size=0.33, random_state=23)

"""##Churn"""

X = df_dados_contas_pre_processados.values
y_churn = marcacoes_churn.values

X_treino_churn, X_teste_churn, y_treino_churn, y_teste_churn = train_test_split(X, y_churn.ravel(), test_size=0.33, random_state=23)

"""##Upselling"""

X = df_dados_contas_pre_processados.values
y_upselling = marcacoes_upselling.values

X_treino_upselling, X_teste_upselling, y_treino_upselling, y_teste_upselling = train_test_split(X, y_upselling.ravel(), test_size=0.33, random_state=23)

""">Beleza, agora já temos divisões de treino e teste para cada um dos nossos targets, podemos começar a ensinar de fato a nossa máquina.

>Mas ensinar como? Bom, eu disse que a máquina entendia números, então, por lidarem com números, a matemática e a probabilidade vão nos ajudar bastante. 

>O modo como ensinamos uma máquina é usando algumas funções matemáticas que configuram um **modelo probabilístico**. 

>O que nos vamos fazer não é criar esse modelo do 0, mas usar alguns modelos que o scikit learn já disponibiliza para a gente.

>Vamos passar os dados de treino e teste para esse modelo e depois entender o quanto conseguimos predizer as nossas targets.

>Vamos começar com a target appetency.

#Criando, treinando e medindo os modelos


---

>O primeiro modelo que vamos criar é chamado de **complementar naive bayes** ou *Complement NB*. Vamos usá-lo pois ele se adapta bem à dados desbalanceados, que é o nosso caso.

>Quando vamos dizer se uma conta considerou algum produto da Orange com appetency ou não, estamos fazendo uma classificação desse produto, certo? Se ele tem ou não appetency. Portando, após criarmos o modelo com **ComplementNB()**, vamos chamá-lo de **classificador**, ou na sua versão abreviada, de **clf**.

##Appetency

###Predição com Complement NB

>Após criarmos o nosso classificador, precisamos passar os dados de treino para ele aprendê-los, fazemos isso **adequando** o classificador à esses dados com **fit**.

>Feito isso, vamos adivinhar ou **prever** os resultados para o teste com **predict**.
"""

from sklearn.naive_bayes import ComplementNB

clf = ComplementNB()
clf.fit(X_treino_appetency, y_treino_appetency.ravel())

predicao_complement_nb_appetency = clf.predict(X_teste_appetency)

""">Agora nos já ensinamos e testamos o classificador. Como vamos saber se ele é bom?

>No desafio, a Orange diz que a métrica para saber o quão bem passamos no teste, ou o quanto a nossa predição é boa é a **AUC**. 

>Mas o que é a AUC? 

>Quando classificamos os nossos dados, podemos ter uma probabilidade alta de termos uma determinada marcação para eles, mas a marcação real ser outra,  o que chamamos de  **falso positivo**, assim como termos uma marcação que não é a marcação real deles com uma probabilidade muito alta, ou **falsos negativos**. O modo de saber sobre esses falsos positivos e negativos é com algo chamado **característica do receptor da informação** ou **ROC**.

>E, quando fazemos um gráfico, a ROC é uma curva, por ela ser uma curva, tem uma área debaixo dessa curva. Essa área é a AUC, do ingês *area under de curve* ou **área debaixo da curva**.

>Mas e aí, quanto será que o nosso classificador é bom? Para isso, vamos calcular a AUC.

###Métrica AUC com Complement NB

>Iremos importar a biblioteca de métricas do scikit learn e calcular a curva ROC para a appetency com o complementNB. Esse cálculo nos devolve os falsos positivos, os positivos verdadeiros e os valores limite da curva. 

>Feito isso, vamos usar os falsos positivos e os verdadeiros positivos para calcular a AUC, e pronto, sabemos que 0.645, ou 65% das vezes o nosso classificador acertou a classificação.

>Então sabemos que o nosso classificador ficou um pouco acima da média ou dos 50%, o que podemos melhorar. Por isso, vamos considerar o Complement NB como o classificador base. Aquele ao qual vamos comparar os dados dos outros classificadores.
"""

from sklearn import metrics

falso_positivo_cnb_appetency, positivo_cnb_appetency, valores_limite_cnb_appetency = metrics.roc_curve(y_teste_appetency, predicao_complement_nb_appetency)
auc_complement_nb_appetency = metrics.auc(falso_positivo_cnb_appetency, positivo_cnb_appetency)
print(auc_complement_nb_appetency)

"""### Plotando a curva ROC do Complement NB

>Para visualizarmos a curva ROC, basta passarmos os falsos e verdadeiros positivos, como fizemos aqui.

>Podemos ver que não fomos muito bem nesse teste, pois uma boa curva ROC ficará próxima do ponto 1.0 que fica no eixo vertical acima e a esquerda.
"""

plt.plot([0, 1], [0, 1], 'k--')
plt.plot(falso_positivo_cnb_appetency, positivo_cnb_appetency)
plt.xlabel('Falsos Positivos')
plt.ylabel('Valores Positivos')
plt.title('Curva ROC Appetency')
plt.show()

""">Temos o nosso classificador base agora, e podemos inclusive repeti-lo para nossos outros targets, só que algo pode ir acontecendo com a nossa métrica.

>A medida em que vamos treinando o nosso modelo, ele vai aprendendo as respostas, e as aprende tão bem que pode decorá-las, sempre acertando o resultado.

>Quando isso acontece, chamamos de **sobreajuste** ou *overfitting*. 

>E qual o problema do overfitting? Quando o classificador se ajusta demais aos dados de treino, ele não consegue entender outros dados, fica viciado, então ele começa a se sair muito mal nos testes.

>Mas temos um jeito de resolver isso. Uma das estratégias para evitar o overfitting é uma técnica chamada **validação cruzada** ou *cross validation*.

>A cross validation, abreviada como **cv**, divide os dados de treino em grupos menores, pega parte desses grupos para treinar o classificador e testa o modelo com os grupos que sobram.  

>Ao fazer isso, garantimos que uma parcela menor dos dados de treinamento sejam dados ao classificador, evitando que ele se vicie, e garantimos que ele será avaliado com base nos dados de treino e teste.

>Até agora, temos a métrica AUC calculada apenas para appetency e classificada com o nosso classificador base, mas podemos usar cross validation com ele, ver os resultados para a AUC e repetir para as outras marcações.

### Resultado com cross validation

>Para usar cross validation, vamos importá-lo do sklearn.

>Criamos também nosso classificador, passamos os valores de treino e teste e dissemos que queríamos a divisão em 5 grupos. Aqui também já podemos calcular a AUC dentro do cross validation, passando que a métrica (*scoring*) vai ser a AUC.

>Vou salvar os resultados em **resultados_cvs_appetency** (cvs de cross validation score), isso porque a validacao cruzada devolve um resultado para cada grupo. 

>Feito isso, para saber o quão bem fomos, iremos fazer a média desses resultados com **.mean()**.
"""

from sklearn.model_selection import cross_val_score

clf = ComplementNB()

resultados_cvs_appetency = cross_val_score(clf, X_treino_appetency, y_treino_appetency.ravel(), cv=5, scoring='roc_auc')

media_resultados_cvs_appetency = resultados_cvs_appetency.mean()

media_resultados_cvs_appetency

""">Feito isso, já sabemos como nos prevenir do overfitting, mas não precisamos usar apenas o classificador base, podemos testar outros tipos de classificadores para saber se o resultado deles não chega mais próximo de 1 (equivalente à 100%).

>Para fazermos isso, vou criar um vetor para guardarmos esses resultados para appetency, salvando nele a média dos resultados do cross validation com o nome de **complement_nb**.
"""

resultados_cross_validation_appetency = []

resultados_cross_validation_appetency.append(('Complement NB', media_resultados_cvs_appetency))

""">Agora podemos ver como nos saímos com outros classificadores. 

>Um outro classificador que podemos usar para comparar com a nossa base é o **Ada Boost**. Esse classificador permite que a gente ajuste configurações internas para classificarmos os nossos dados ainda melhor.

>Ele faz isso criando cópias dele mesmo e treinando com os nossos dados, mas entre uma cópia e outra, entende o que não foi tão bem na classificação e ajusta esses valores para que a próxima seja melhor. Esses valores são os **pesos** ou *weights* do nosso modelo. 

>Para usarmos o Ada Boost, faremos da mesma maneira que o Complement NB. Impotando o classificador, criando o classificador, fazendo a validação cruzada, tirando a média dos resultados e guardando no array.

### Cross validation com Adaboost
"""

from sklearn.ensemble import AdaBoostClassifier

clf = AdaBoostClassifier()

resultados_adab = cross_val_score(clf, X_treino_appetency, y_treino_appetency.ravel(), cv=5, scoring='roc_auc')

media_resultados_adab_appetency = resultados_adab.mean()

resultados_cross_validation_appetency.append(('Ada Boost', media_resultados_adab_appetency))

""">Bacana, agora temos os dados de dois classificadores, vamos usar um último para desempatar.

>Para esse último, eu escolhi o **Random Forest**. Esse modelo é mais complexo do que os outros, pois ele usa uma estrutura que vai fazendo combinações de todos os nossos dados, o que custa mais computacionalmente.

>Essa estrutura fixa um ponto de início e vai combinando com nossos outros dados, e assim por diante, como se fossem galhos de uma árvore se ramificando, até mesmo por isso, essa estrutura é chamada de **árvore**.

>Mas o nosso modelo não faz só isso, o Random Forest também melhora as predições porque pega grupos menores dos nossos dados, aplica os classificadores de ávores, e vai fazendo a média dos resultados deles. 

>Além disso, os dados vão sendo trocados ou permutados a cada vez que a árvore ramifica, o que ajuda os nossos resultados a variarem bastante. E como vamos saber como o nosso modelo está se saindo, se o seu resultado muda a todo momento?

>Ainda, precisamos comparar esses resultados para saber qual classificador está se saindo melhor. Como vamos comparar o Ada Boost com o Random Forest se em um momento seu resultado dá 0.7 e em outro 0.9?

>Então é interessante que tenhamos sempre o mesmo resultado para podermos avaliar.

>Para conseguir isso, iremos definir um estado aleatório fixo, ou **random_state**.

>Então vamos criar o nosso classificador e repetir os passos anteriores.

### Cross validation com random forest

>Aqui também vamos definir o número de árvores (lembra daquelas cópias que ele faz dele mesmo?) como 100.

>Escolhemos esse número pois ele será o novo padrão da versão atualizada do modelo, mas quando você trabalha com o random forest, é sempre bom ajustar os seus parâmetros para que você não fique com uma quantidade muito grande de árvores ou de ramificações e que ele demore bastante para encontrar uma resposta.
"""

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(random_state=23, n_estimators=100)

resultados_rfc = cross_val_score(clf, X_treino_appetency, y_treino_appetency.ravel(), cv=5, scoring='roc_auc')

media_resultados_rfc_appetency = resultados_rfc.mean()

resultados_cross_validation_appetency.append(('Random Forest', media_resultados_rfc_appetency))

"""###Ordenando os resultados para os modelos de appetency com cv

>Agora que temos 3 classificadores, podemos saber qual foi o melhor resultado para o nosso target appetency.

>Para isso, vamos pegar os nossos resultados, e ordená-los com **sorted**.

>Como a posição 0 dos resultados é o nome dos modelos, iremos ordená-los pela métrica, que é o número que está na posição 1.

>Para fazer isso, vamos passar como um critério de *sorted* uma função anônima **lambda** que pegará a primeira posição de cada um dos resultados.
"""

resultados_ordenados_cross_validation_appetency = sorted(resultados_cross_validation_appetency, key=lambda x: x[1], reverse=True)

resultados_ordenados_cross_validation_appetency

""">Agora que vimos qual o melhor modelo para o nosso target appetency, podemos ver também quais são os resultados para upselling e churn.

>Para facilitar, vou fazer uma função que calcula a métrica por classificador e chamá-la, assim não precisamos repetir código.

## Resultados para upselling e churn com validação cruzada


---


Cálculo e exibição dos resultados para Ada Boost, Naive Bayes Complement e Random Forest.
"""

def calcula_metrica_por_classificador (X_treino, y_treino):
  
  classificadores = [
      ('Naive Bayes Complement', ComplementNB()),
      ('Ada Boost', AdaBoostClassifier()),
      ('Random Forest', RandomForestClassifier(random_state=23, n_estimators=100))
  ]
  
  resultados = []

  for nome_do_modelo, modelo in classificadores:
     
    resultados_cv = cross_val_score(modelo, X_treino, y_treino.ravel(), cv=5, scoring='roc_auc')   
    media_resultados_cv = resultados_cv.mean() 
    resultados.append((nome_do_modelo, media_resultados_cv))
  
  resultados_ordenados_decrescentes = sorted(resultados, key=lambda x: x[1], reverse=True)
    
  return resultados_ordenados_decrescentes

resultados_ordenados_cross_validation_upselling = calcula_metrica_por_classificador(X_treino_upselling, y_treino_upselling)

resultados_ordenados_cross_validation_churn = calcula_metrica_por_classificador(X_treino_churn, y_treino_churn)

resultados_ordenados_cross_validation_upselling

resultados_ordenados_cross_validation_churn

resultados_ordenados_cross_validation_appetency

""">Agora calculamos e temos os resultados para nossos 3 targets, o modelo que vamos escolher vai ser aquele cujo resultado mais se aproxima de 1.

###Vendo o que acontece com a primeira predição de churn
"""

clf = AdaBoostClassifier()
clf.fit(X_treino_churn, y_treino_churn.ravel())
y_predicao_churn = clf.predict(X_teste_churn)

y_predicao_churn[0]

y_teste_churn[0]